{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded initial model\n",
      "WARNING:tensorflow:From C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\site-packages\\keras\\engine\\training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n",
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 690, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 685, in <lambda>\n",
      "    initargs=(seqs, self.random_seed))\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\multiprocessing\\context.py\", line 119, in Pool\n",
      "    context=self.get_context())\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\multiprocessing\\pool.py\", line 174, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\multiprocessing\\pool.py\", line 239, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\multiprocessing\\process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\multiprocessing\\context.py\", line 322, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\multiprocessing\\popen_spawn_win32.py\", line 65, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "TypeError: can't pickle generator objects\n",
      "\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 690, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 685, in <lambda>\n",
      "    initargs=(seqs, self.random_seed))\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\multiprocessing\\context.py\", line 119, in Pool\n",
      "    context=self.get_context())\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\multiprocessing\\pool.py\", line 174, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\multiprocessing\\pool.py\", line 239, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\multiprocessing\\process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\multiprocessing\\context.py\", line 322, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\multiprocessing\\popen_spawn_win32.py\", line 65, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\Users\\guilh\\anaconda3\\envs\\iris\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "TypeError: can't pickle generator objects\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import helperFunc\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import ImageEnhance,ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.layers.core import Lambda\n",
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint,Callback\n",
    "from keras.activations import relu\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, GlobalAveragePooling2D ,Dense, Concatenate,Conv2DTranspose,Conv2D,Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications import densenet\n",
    "\n",
    "\n",
    "# Putting this in for some errors that occur on my setup\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "# np.random.seed(1337) # for reproducibility\n",
    "\n",
    "#Number of classes\n",
    "classes = 293\n",
    "\n",
    "#Length of feature vector\n",
    "FEATURES = 1024\n",
    "\n",
    "#Image dimensions, square images only, can change to non-square\n",
    "imgDimension = 256\n",
    "\n",
    "#Dataset location\n",
    "datasetLoc = 'training_dataset_folder/*'\n",
    "\n",
    "#Batch_size for training, can fit 12 in a GTX 1070 with ResNet 50\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "#Num of images per class, for batch hard training\n",
    "im_per_class = 4 \n",
    "\n",
    "\n",
    "#Sorting for file reads\n",
    "numbers = re.compile(r'(\\d+)')\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    \"\"\"\n",
    "    Rotates an OpenCV 2 / NumPy image about it's centre by the given angle\n",
    "    (in degrees). The returned image will be large enough to hold the entire\n",
    "    new image, with a black background, taken from stackoverflow (https://stackoverflow.com/questions/9041681/opencv-python-rotate-image-by-x-degrees-around-specific-point)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the image size\n",
    "    # No that's not an error - NumPy stores image matricies backwards\n",
    "    image_size = (image.shape[1], image.shape[0])\n",
    "    image_center = tuple(np.array(image_size) / 2)\n",
    "\n",
    "    # Convert the OpenCV 3x2 rotation matrix to 3x3\n",
    "    rot_mat = np.vstack(\n",
    "        [cv2.getRotationMatrix2D(image_center, angle, 1.0), [0, 0, 1]]\n",
    "    )\n",
    "\n",
    "    rot_mat_notranslate = np.matrix(rot_mat[0:2, 0:2])\n",
    "\n",
    "    # Shorthand for below calcs\n",
    "    image_w2 = image_size[0] * 0.5\n",
    "    image_h2 = image_size[1] * 0.5\n",
    "\n",
    "    # Obtain the rotated coordinates of the image corners\n",
    "    rotated_coords = [\n",
    "        (np.array([-image_w2,  image_h2]) * rot_mat_notranslate).A[0],\n",
    "        (np.array([ image_w2,  image_h2]) * rot_mat_notranslate).A[0],\n",
    "        (np.array([-image_w2, -image_h2]) * rot_mat_notranslate).A[0],\n",
    "        (np.array([ image_w2, -image_h2]) * rot_mat_notranslate).A[0]\n",
    "    ]\n",
    "\n",
    "    # Find the size of the new image\n",
    "    x_coords = [pt[0] for pt in rotated_coords]\n",
    "    x_pos = [x for x in x_coords if x > 0]\n",
    "    x_neg = [x for x in x_coords if x < 0]\n",
    "\n",
    "    y_coords = [pt[1] for pt in rotated_coords]\n",
    "    y_pos = [y for y in y_coords if y > 0]\n",
    "    y_neg = [y for y in y_coords if y < 0]\n",
    "\n",
    "    right_bound = max(x_pos)\n",
    "    left_bound = min(x_neg)\n",
    "    top_bound = max(y_pos)\n",
    "    bot_bound = min(y_neg)\n",
    "\n",
    "    new_w = int(abs(right_bound - left_bound))\n",
    "    new_h = int(abs(top_bound - bot_bound))\n",
    "\n",
    "    trans_mat = np.matrix([\n",
    "        [1, 0, int(new_w * 0.5 - image_w2)],\n",
    "        [0, 1, int(new_h * 0.5 - image_h2)],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    # Compute the tranform for the combined rotation and translation\n",
    "    affine_mat = (np.matrix(trans_mat) * np.matrix(rot_mat))[0:2, :]\n",
    "\n",
    "    # Apply the transform\n",
    "    result = cv2.warpAffine(\n",
    "        image,\n",
    "        affine_mat,\n",
    "        (new_w, new_h),\n",
    "        flags=cv2.INTER_LINEAR\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "    Augment function, needs an image and an augment type integer.\n",
    "'''\n",
    "def augment(im,augType):\n",
    "        if(augType == 0):\n",
    "            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "            im = cv2.filter2D(im, -1, kernel)\n",
    "        if(augType == 1):\n",
    "            im = rotate_image(im,30)\n",
    "            im =cv2.resize(im,(imgDimension,imgDimension))\n",
    "        if(augType == 2):\n",
    "            im = rotate_image(im,120)\n",
    "            im =cv2.resize(im,(imgDimension,imgDimension))\n",
    "        if(augType == 3):\n",
    "            im = rotate_image(im,320)\n",
    "            im =cv2.resize(im,(imgDimension,imgDimension))\n",
    "        if(augType == 4):\n",
    "            im = rotate_image(im,230)\n",
    "            im =cv2.resize(im,(imgDimension,imgDimension))\n",
    "        if(augType == 5):\n",
    "            im = np.flip(im,0)\n",
    "        if(augType == 6):\n",
    "            im = np.flip(im,1)\n",
    "        if(augType == 7):\n",
    "            im[0:70] = 0\n",
    "        if(augType == 8):\n",
    "            cl1 = clahe.apply(im[:,:,0])\n",
    "            cl1[cl1<10] = 0\n",
    "            im[:,:,0] = cl1\n",
    "            im[:,:,1] = cl1\n",
    "            im[:,:,2] = cl1\n",
    "        if (augType == 9):\n",
    "            im = cv2.blur(im, (3,3))\n",
    "        return im\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Keras generator for loading images in a batch hard fashion. Multithreaded by default in keras.\n",
    "    Modeled for a Tensorflow backend. Takes a static string to the dataset location.\n",
    "\"\"\"\n",
    "\n",
    "folderList = sorted(glob.glob(datasetLoc),key=numericalSort)\n",
    "\n",
    "def customGenerator():\n",
    "    CLASSES = classes\n",
    "    K = im_per_class\n",
    "    batchSize = BATCH_SIZE\n",
    "    #Static paths for now\n",
    "\n",
    "    yConcat = np.zeros(shape = (batchSize,1+FEATURES),dtype = np.int32)\n",
    "    \n",
    "    while True:\n",
    "        y2 = np.zeros(shape = (1+FEATURES),dtype = np.int32)\n",
    "        xI = np.zeros(shape = (batchSize,3,imgDimension,imgDimension))\n",
    "        numFm = random.sample(range(CLASSES), batchSize//K)\n",
    "        for x in range(batchSize//K):\n",
    "            imList = glob.glob(folderList[numFm[x]] + \"/*\")\n",
    "            numIm = random.sample(range(len(imList)), K)\n",
    "            #Augmentation, comment out if not needed\n",
    "            augType = random.sample(range(9), 1)\n",
    "            for i in range(len(numIm)):\n",
    "                im = cv2.imread(imList[numIm[i]])\n",
    "                im = cv2.resize(im,(imgDimension,imgDimension))\n",
    "                im= augment(im,augType)\n",
    "                im= np.einsum('lij->jli', im)\n",
    "                xI[i + (x*K)] = im\n",
    "                yConcat [i + (x*K)] = y2\n",
    "                yConcat [i + (x*K)][-1] = numFm[x]\n",
    "\n",
    "        xI = np.einsum('abcd->acdb',xI)\n",
    "        yield (xI,yConcat)\n",
    "        \n",
    "'''\n",
    "asoftmax loss for keras, takes in the true and predicted logits. \n",
    "Feature vector is passed to this loss function, logits calculated outside.\n",
    "Works on concatenated logit/feature vector list, taken from https://github.com/pppoe/tensorflow-sphereface-asoftmax on github\n",
    "'''\n",
    "def Loss_ASoftmax(true, pred):\n",
    "    '''\n",
    "    x: B x D - data\n",
    "    y: B x 1 - label\n",
    "    l: 1 - lambda \n",
    "    '''\n",
    "    m = 4\n",
    "    num_cls = CLASSES\n",
    "    x = pred[:,CLASSES:]\n",
    "    yX = true[:,-1]\n",
    "    y = tf.cast(yX, tf.int32)\n",
    "    l = 1.0\n",
    "    eps = 1e-8\n",
    "\n",
    "    logits = pred[:,:CLASSES]\n",
    "    if m == 0:\n",
    "\n",
    "        return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "    ordinal = tf.constant(list(range(0, BATCH_SIZE)))\n",
    "    ordinal_y = tf.stack([ordinal, y], axis = 1)\n",
    "\n",
    "    x_norm = tf.norm(x, axis = 1) + eps\n",
    "    print(x_norm)\n",
    "    sel_logits = tf.gather_nd(logits, ordinal_y)\n",
    "\n",
    "    cos_th = tf.div(sel_logits, x_norm)\n",
    "    if m == 1:\n",
    "\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "    else:\n",
    "\n",
    "        if m == 2:\n",
    "\n",
    "            cos_sign = tf.sign(cos_th)\n",
    "            res = 2*tf.multiply(tf.sign(cos_th), tf.square(cos_th)) - 1\n",
    "\n",
    "        elif m == 4:\n",
    "\n",
    "            cos_th2 = tf.square(cos_th)\n",
    "            cos_th4 = tf.pow(cos_th, 4)\n",
    "            sign0 = tf.sign(cos_th)\n",
    "            sign3 = tf.multiply(tf.sign(2*cos_th2 - 1), sign0)\n",
    "            sign4 = 2*sign0 + sign3 - 3\n",
    "            res = sign3*(8*cos_th4 - 8*cos_th2 + 1) + sign4\n",
    "        else:\n",
    "            raise ValueError('unsupported value of m')\n",
    "\n",
    "        scaled_logits = tf.multiply(res, x_norm)\n",
    "\n",
    "        f = 1.0/(1.0+l)\n",
    "        ff = 1.0 - f\n",
    "        blah = tf.subtract(scaled_logits, sel_logits)\n",
    "        print (blah)\n",
    "        print (ordinal_y)\n",
    "        b = tf.constant([BATCH_SIZE,CLASSES])\n",
    "\n",
    "        blah1 = tf.scatter_nd(ordinal_y,blah , b)\n",
    "        comb_logits_diff = tf.add(logits, blah1) \n",
    "        updated_logits = ff*logits + f*comb_logits_diff\n",
    "        updated_logits = updated_logits*32\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=updated_logits))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Batch hard loss implementation from https://omoindrot.github.io/triplet-loss\n",
    "    Modified to use a soft margin.\n",
    "\"\"\"\n",
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i and j are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "\n",
    "    # Check if labels[i] == labels[j]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(indices_not_equal, labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "\n",
    "    return mask\n",
    "def _get_triplet_mask(labels):\n",
    "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
    "    A triplet (i, j, k) is valid if:\n",
    "        - i, j, k are distinct\n",
    "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i, j and k are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "\n",
    "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "\n",
    "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
    "\n",
    "    return mask\n",
    "\n",
    "def batch_hard_triplet_loss(true, pred, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = pred[:,:-1]\n",
    "    labels = true[:,-1]\n",
    "    margin = 0.8\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # For each anchor, get the hardest positive\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "    mask_anchor_positive = tf.to_float(mask_anchor_positive)\n",
    "\n",
    "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
    "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
    "\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
    "\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    mask_anchor_negative = tf.to_float(mask_anchor_negative)\n",
    "\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "\n",
    "    # shape (batch_size,)\n",
    "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
    "#     hP = K.max(hardest_positive_dist)\n",
    "#     hN =  K.min(hardest_negative_dist)\n",
    "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "    #Usual batch hard triplet loss:\n",
    "#     triplet_loss = tf.maximum(hP - hN + margin, 0.0)\n",
    "\n",
    "    #Soft-margin triplet loss\n",
    "    triplet_loss = K.log(1.0 + K.exp(hardest_positive_dist - hardest_negative_dist))\n",
    "    # Get final mean triplet loss\n",
    "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
    "\n",
    "    return triplet_loss\n",
    "def batch_all_triplet_loss(true, pred, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    We generate all the valid triplets and average the loss over the positive ones.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "#     print (labels)\n",
    "    embeddings = pred[:,:-1]\n",
    "    labels = true[:,-1]\n",
    "    \n",
    "#     print (embeddings)\n",
    "    margin = 0.7\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    anchor_positive_dist = tf.expand_dims(pairwise_dist, 2)\n",
    "    anchor_negative_dist = tf.expand_dims(pairwise_dist, 1)\n",
    "\n",
    "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
    "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
    "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
    "    # and the 2nd (batch_size, 1, batch_size)\n",
    "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
    "\n",
    "    # Put to zero the invalid triplets\n",
    "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
    "    mask = _get_triplet_mask(labels)\n",
    "    mask = tf.to_float(mask)\n",
    "    triplet_loss = tf.multiply(mask, triplet_loss)\n",
    "\n",
    "    # Remove negative losses (i.e. the easy triplets)\n",
    "    triplet_loss = tf.maximum(triplet_loss, 0.0)\n",
    "\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    valid_triplets = tf.to_float(tf.greater(triplet_loss, 1e-16))\n",
    "    num_positive_triplets = tf.reduce_sum(valid_triplets)\n",
    "    num_valid_triplets = tf.reduce_sum(mask)\n",
    "    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + 1e-16)\n",
    "\n",
    "    # Get final mean triplet loss over the positive valid triplets\n",
    "    triplet_loss = tf.reduce_sum(triplet_loss) / (num_positive_triplets + 1e-16)\n",
    "\n",
    "    return triplet_loss\n",
    "\n",
    "def _pairwise_distances(embeddings, squared=False):\n",
    "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    dot_product = tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    square_norm = tf.diag_part(dot_product)\n",
    "\n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "#     distances = tf.expand_dims(square_norm, 0) - 2.0 * dot_product + tf.expand_dims(square_norm, 1)\n",
    "    distances =  (dot_product/tf.expand_dims(square_norm, 0)*tf.expand_dims(square_norm, 1))\n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = 1-distances\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "\n",
    "    if not squared:\n",
    "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
    "        # we need to add a small epsilon where distances == 0.0\n",
    "        mask = tf.to_float(tf.equal(distances, 0.0))\n",
    "        distances = distances + mask * 1e-8\n",
    "\n",
    "        distances = tf.sqrt(distances)\n",
    "\n",
    "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
    "        distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances\n",
    "\n",
    "def l2Norm(x):\n",
    "    return  K.l2_normalize(x, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Model generation\"\"\"\n",
    "inp = Input(shape = (3,imgDimension,imgDimension))\n",
    "base_model = densenet.DenseNet169(weights='imagenet', include_top=False,input_shape=(3,imgDimension,imgDimension))\n",
    "\n",
    "\n",
    "print (\"Loaded initial model\")\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "fc2 = Dense(FEATURES,activation = 'relu',name = 'embedding_weights',use_bias = False)(x)\n",
    "fc2 = BatchNormalization()(fc2)\n",
    "out = Lambda(lambda x: K.l2_normalize(x, axis=1))(fc2)\n",
    "label = Dense(1,activation='linear',name = 'label')(out)\n",
    "conc = concatenate([fc2,label], name='xF')\n",
    "triplet_model = Model(inputs=base_model.input, outputs=conc)\n",
    "\n",
    "\"\"\"Model generation\"\"\"\n",
    "\n",
    "cwd = os.getcwd()\n",
    "filepath=cwd + \"/batchHard_ResNet50-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_predictions_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "callbacks_list = [checkpoint]\n",
    "# print (triplet_model.summary())\n",
    "\n",
    "# triplet_model.load_weights(\"/home/sohaib/Downloads/Features/batchHard_ResNet50-08-0.58.hdf5\",by_name = True)\n",
    "\"\"\"\n",
    "    Training happens here, same training and validation generators due to laziness\n",
    "\"\"\"\n",
    "gen_tr = customGenerator()\n",
    "gen_te = customGenerator()\n",
    "triplet_model.compile(loss=batch_hard_triplet_loss, optimizer=SGD(0.0009))\n",
    "triplet_model.fit_generator(gen_tr,validation_data=gen_te,  \n",
    "                          epochs=7, \n",
    "                          verbose=1,\n",
    "                          workers=4,\n",
    "                          steps_per_epoch=300, \n",
    "                          validation_steps=50,use_multiprocessing = True,callbacks = callbacks_list)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Folder to infer stuff on\n",
    "\"\"\"\n",
    "ROOT_DIR = os.getcwd()\n",
    "imList = sorted(glob.glob(ROOT_DIR+ \"/Folder_to_infer_on/*\"),key=numericalSort)\n",
    "print (imList[0])\n",
    "print (len(imList))\n",
    "\n",
    "\n",
    "p = 8#multiprocessing cpu_count\n",
    "pool = Pool(processes=8)\n",
    "print (\"Processors Available = \" + str(p))\n",
    "\n",
    "\n",
    "lst = helperFunc.prepareFeatures(triplet_model,imList,imgDimension,FEATURES)\n",
    "lst = np.asarray(lst)\n",
    "\n",
    "\n",
    "\n",
    "activations = lst\n",
    "\n",
    "\n",
    "# \"\"\" \n",
    "#     Hardcoded stuff for multi core :p\n",
    "# \"\"\"\n",
    "lst = pool.map(helperFunc.featureSelect,((activations,imList,1),(activations,imList,2),\n",
    "                              (activations,imList,3),(activations,imList,4),(activations,imList,5),\n",
    "                              (activations,imList,6),(activations,imList,7),(activations,imList,8)))\n",
    "\n",
    "tP = 0\n",
    "masterBlue = []\n",
    "masterRed = []\n",
    "masterVar = np.zeros(shape = (FEATURES*3))\n",
    "for x in range(len(lst)):\n",
    "    for y in range(len(lst[x][0])):\n",
    "        masterBlue.append(lst[x][0][y])\n",
    "    for z in range(len(lst[x][1])):\n",
    "        masterRed.append(lst[x][1][z])\n",
    "for x in range(len(lst)):\n",
    "        tP+= lst[x][2]\n",
    "print (\"Total true positives = \" + str(tP))\n",
    "masterRed = np.sort(masterRed)\n",
    "plt.hist(masterBlue, normed=True, bins=120, histtype='stepfilled', color='b',alpha=0.7, label='Same')\n",
    "plt.hist(masterRed, normed=True, bins=120, histtype='stepfilled', color='r', label='Different')\n",
    "plt.show()\n",
    "\n",
    "print (\"Standard deviation same = \" + str(np.std(masterBlue)))\n",
    "print (\"Standard deviation different = \" + str(np.std(masterRed)))\n",
    "print (\"Blah = \" + str(len(masterBlue)))\n",
    "print (\"Min Same class = \" + str(min(masterBlue)))\n",
    "print (\"Min Different class = \" + str(min(masterRed)))\n",
    "print (\"Minimum = \" + str(masterRed[:60]))\n",
    "print (\"Max = \" + str(masterRed[-60:]))\n",
    "\n",
    "print (\"Average Same class = \" + str(sum(masterBlue) / float(len(masterBlue))))\n",
    "print (\"Average Different class = \" + str(sum(masterRed) / float(len(masterRed))))\n",
    "\n",
    "\n",
    "# u = np.mean(masterRed)\n",
    "# entropy = (u*(1-u))/np.var(masterRed)\n",
    "# print (\"Entropy = \" + str(entropy))\n",
    "\n",
    "blueFile = open('blue.txt', 'w')\n",
    "for item in masterBlue:\n",
    "    blueFile.write(\"%s\\n\" % item)\n",
    "\n",
    "redFile = open('red.txt', 'w')\n",
    "for item in masterRed:\n",
    "    redFile.write(\"%s\\n\" % item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (iris)",
   "language": "python",
   "name": "iris"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
